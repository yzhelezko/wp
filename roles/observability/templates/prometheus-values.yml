rbac:
  create: true

podSecurityPolicy:
  enabled: false

imagePullSecrets:
# - name: "image-pull-secret"

## Define serviceAccount names for components. Defaults to component's fully qualified name.
##
serviceAccounts:
  alertmanager:
    create: true
    name:
    annotations: {}
  nodeExporter:
    create: true
    name:
    annotations: {}
  pushgateway:
    create: true
    name:
    annotations: {}
  server:
    create: true
    name:
    annotations: {}

alertmanager:
  ## If false, alertmanager will not be installed
  ##
  enabled: true

  ## Use a ClusterRole (and ClusterRoleBinding)
  ## - If set to false - we define a Role and RoleBinding in the defined namespaces ONLY
  ## This makes alertmanager work - for users who do not have ClusterAdmin privs, but wants alertmanager to operate on their own namespaces, instead of clusterwide.
  useClusterRole: true

  ## Set to a rolename to use existing role - skipping role creating - but still doing serviceaccount and rolebinding to the rolename set here.
  useExistingRole: false

  ## alertmanager container name
  ##
  name: alertmanager

  ## alertmanager container image
  ##
  image:
    repository: prom/alertmanager
    tag: v0.21.0
    pullPolicy: IfNotPresent

  ## alertmanager priorityClassName
  ##
  priorityClassName: ""

  ## Additional alertmanager container arguments
  ##
  extraArgs: {}

  ## Additional InitContainers to initialize the pod
  ##
  extraInitContainers: []

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  prefixURL: ""

  ## External URL which can access alertmanager
  baseURL: "http://{{ domain }}{{ observability_subpathAlertmanager }}"

  ## Additional alertmanager container environment variable
  ## For instance to add a http_proxy
  ##
  extraEnv: {}

  ## Additional alertmanager Secret mounts
  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
  extraSecretMounts: []
    # - name: secret-files
    #   mountPath: /etc/secrets
    #   subPath: ""
    #   secretName: alertmanager-secret-files
    #   readOnly: true

  ## ConfigMap override where fullname is .Release.Name-.Values.alertmanager.configMapOverrideName
  ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configMapOverrideName: "alertmanager-cm"

  ## The name of a secret in the same kubernetes namespace which contains the Alertmanager config
  ## Defining configFromSecret will cause templates/alertmanager-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configFromSecret: ""

  ## The configuration file name to be loaded to alertmanager
  ## Must match the key within configuration loaded from ConfigMap/Secret
  ##
  configFileName: alertmanager.yml

  ingress:
    ## If true, alertmanager Ingress will be created
    ##
    enabled: false

    ## alertmanager Ingress annotations
    ##
    annotations:
      kubernetes.io/ingress.class: "{{ ingressClass }}"
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## alertmanager Ingress additional labels
    ##
    extraLabels: {}

    ## alertmanager Ingress hostnames with optional path
    ## Must be provided if Ingress is enabled
    ##
    hosts:
      - "{{ domain }}{{ observability_subpathAlertmanager }}"
    #   - alertmanager.domain.com
    #   - domain.com/alertmanager

    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /*
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation

    ## alertmanager Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls:
     - secretName: tls-domain-certificate
       hosts:
       - "{{ domain }}"


  ## Alertmanager Deployment Strategy type
  # strategy:
  #   type: Recreate

  ## Node tolerations for alertmanager scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for alertmanager pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Pod affinity
  ##
  affinity: {}

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  ##
  podDisruptionBudget:
    enabled: false
    maxUnavailable: 1

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  persistentVolume:
    ## If true, alertmanager will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## alertmanager data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## alertmanager data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## alertmanager data Persistent Volume existing claim name
    ## Requires alertmanager.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## alertmanager data Persistent Volume mount root path
    ##
    mountPath: /data

    ## alertmanager data Persistent Volume size
    ##
    size: 5Gi

    ## alertmanager data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## alertmanager data Persistent Volume Binding Mode
    ## If defined, volumeBindingMode: <volumeBindingMode>
    ## If undefined (the default) or set to null, no volumeBindingMode spec is
    ##   set, choosing the default mode.
    ##
    # volumeBindingMode: ""

    ## Subdirectory of alertmanager data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  emptyDir:
    ## alertmanager emptyDir volume size limit
    ##
    sizeLimit: ""

  ## Annotations to be added to alertmanager pods
  ##
  podAnnotations: {}
    ## Tell prometheus to use a specific set of alertmanager pods
    ## instead of all alertmanager pods found in the same namespace
    ## Useful if you deploy multiple releases within the same namespace
    ##
    ## prometheus.io/probe: alertmanager-teamA

  ## Labels to be added to Prometheus AlertManager pods
  ##
  podLabels: {}

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)
  ##
  replicaCount: 1

  ## Annotations to be added to deployment
  ##
  deploymentAnnotations: {}

  statefulSet:
    ## If true, use a statefulset instead of a deployment for pod management.
    ## This allows to scale replicas to more than 1 pod
    ##
    enabled: true

    annotations: {}
    labels: {}
    podManagementPolicy: OrderedReady

    ## Alertmanager headless service to use for the statefulset
    ##
    headless:
      annotations: {}
      labels: {}

      ## Enabling peer mesh service end points for enabling the HA alert manager
      ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md
      enableMeshPeer: false

      servicePort: 80

  ## alertmanager resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    limits:
      cpu: 100m
      memory: 48Mi
    requests:
      cpu: 40m
      memory: 32Mi

  ## Security context to be added to alertmanager pods
  ##
  securityContext:
    runAsUser: 65534
    runAsNonRoot: true
    runAsGroup: 65534
    fsGroup: 65534

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## Enabling peer mesh service end points for enabling the HA alert manager
    ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md
    # enableMeshPeer : true

    ## List of IP addresses at which the alertmanager service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    # nodePort: 30000
    sessionAffinity: None
    type: ClusterIP

## Monitors ConfigMap changes and POSTs to a URL
## Ref: https://github.com/jimmidyson/configmap-reload
##
configmapReload:
  prometheus:
    ## If false, the configmap-reload container will not be deployed
    ##
    enabled: true

    ## configmap-reload container name
    ##
    name: configmap-reload

    ## configmap-reload container image
    ##
    image:
      repository: jimmidyson/configmap-reload
      tag: v0.4.0
      pullPolicy: IfNotPresent

    ## Additional configmap-reload container arguments
    ##
    extraArgs: {}
    ## Additional configmap-reload volume directories
    ##
    extraVolumeDirs: []


    ## Additional configmap-reload mounts
    ##
    extraConfigmapMounts: []
      # - name: prometheus-alerts
      #   mountPath: /etc/alerts.d
      #   subPath: ""
      #   configMap: prometheus-alerts
      #   readOnly: true


    ## configmap-reload resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
  alertmanager:
    ## If false, the configmap-reload container will not be deployed
    ##
    enabled: true

    ## configmap-reload container name
    ##
    name: configmap-reload

    ## configmap-reload container image
    ##
    image:
      repository: jimmidyson/configmap-reload
      tag: v0.4.0
      pullPolicy: IfNotPresent

    ## Additional configmap-reload container arguments
    ##
    extraArgs: {}
    ## Additional configmap-reload volume directories
    ##
    extraVolumeDirs: []


    ## Additional configmap-reload mounts
    ##
    extraConfigmapMounts: []
      # - name: prometheus-alerts
      #   mountPath: /etc/alerts.d
      #   subPath: ""
      #   configMap: prometheus-alerts
      #   readOnly: true


    ## configmap-reload resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}

kubeStateMetrics:
  ## If false, kube-state-metrics sub-chart will not be installed
  ##
  enabled: true

## kube-state-metrics sub-chart configurable values
## Please see https://github.com/helm/charts/tree/master/stable/kube-state-metrics
##
kube-state-metrics:
  image:
    repository: quay.io/coreos/kube-state-metrics
    tag: v1.9.7
    pullPolicy: IfNotPresent

nodeExporter:
  ## If false, node-exporter will not be installed
  ##
  enabled: true

  ## If true, node-exporter pods share the host network namespace
  ##
  hostNetwork: true

  ## If true, node-exporter pods share the host PID namespace
  ##
  hostPID: true

  ## node-exporter container name
  ##
  name: node-exporter

  ## node-exporter container image
  ##
  image:
    repository: prom/node-exporter
    tag: v1.0.1
    pullPolicy: IfNotPresent

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  ## node-exporter priorityClassName
  ##
  priorityClassName: ""

  ## Custom Update Strategy
  ##
  updateStrategy:
    type: RollingUpdate

  ## Additional node-exporter container arguments
  ##
  extraArgs: {}

  ## Additional InitContainers to initialize the pod
  ##
  extraInitContainers: []

  ## Additional node-exporter hostPath mounts
  ##
  extraHostPathMounts: []
    # - name: textfile-dir
    #   mountPath: /srv/txt_collector
    #   hostPath: /var/lib/node-exporter
    #   readOnly: true
    #   mountPropagation: HostToContainer

  extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /prometheus
    #   configMap: certs-configmap
    #   readOnly: true

  ## Node tolerations for node-exporter scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for node-exporter pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to node-exporter pods
  ##
  podAnnotations: {}

  ## Labels to be added to node-exporter pods
  ##
  pod:
    labels: {}

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  ##
  podDisruptionBudget:
    enabled: false
    maxUnavailable: 1

  ## node-exporter resource limits & requests
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    limits:
      cpu: 400m
      memory: 128Mi
    requests:
      cpu: 150m
      memory: 64Mi

  ## Security context to be added to node-exporter pods
  ##
  securityContext: {}
    # runAsUser: 0

  service:
    annotations:
      prometheus.io/scrape: "true"
    labels: {}

    # Exposed as a headless service:
    # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
    clusterIP: None

    ## List of IP addresses at which the node-exporter service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    hostPort: 9100
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9100
    type: ClusterIP

server:
  ## Prometheus server container name
  ##
  enabled: true

  ## Use a ClusterRole (and ClusterRoleBinding)
  ## - If set to false - we define a RoleBinding in the defined namespaces ONLY
  ##
  ## NB: because we need a Role with nonResourceURL's ("/metrics") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.
  ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.
  ##
  ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.
  ##
  # useExistingClusterRoleName: nameofclusterrole

  ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.
  # namespaces:
  #   - yournamespace

  name: server
  sidecarContainers:

  ## Prometheus server container image
  ##
  image:
    repository: prom/prometheus
    tag: v2.21.0
    pullPolicy: IfNotPresent

  ## prometheus server priorityClassName
  ##
  priorityClassName: ""

  ## EnableServiceLinks indicates whether information about services should be injected
  ## into pod's environment variables, matching the syntax of Docker links.
  ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0.
  ##
  enableServiceLinks: true

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  prefixURL: ""

  ## External URL which can access prometheus
  ## Maybe same with Ingress host name
  baseURL: ""

  ## Additional server container environment variables
  ##
  ## You specify this manually like you would a raw deployment manifest.
  ## This means you can bind in environment variables from secrets.
  ##
  ## e.g. static environment variable:
  ##  - name: DEMO_GREETING
  ##    value: "Hello from the environment"
  ##
  ## e.g. secret environment variable:
  ## - name: USERNAME
  ##   valueFrom:
  ##     secretKeyRef:
  ##       name: mysecret
  ##       key: username
  env: []

  extraFlags:
    - web.enable-lifecycle
    ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as
    ## deleting time series. This is disabled by default.
    # - web.enable-admin-api
    ##
    ## storage.tsdb.no-lockfile flag controls BD locking
    # - storage.tsdb.no-lockfile
    ##
    ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL)
    # - storage.tsdb.wal-compression

  ## Path to a configuration file on prometheus server container FS
  configPath: /etc/config/prometheus.yml

  global:
    ## How frequently to scrape targets by default
    ##
    scrape_interval: 1m
    ## How long until a scrape request times out
    ##
    scrape_timeout: 10s
    ## How frequently to evaluate rules
    ##
    evaluation_interval: 1m
  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
  ##
  remoteWrite: []
  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read
  ##
  remoteRead: []

  ## Additional Prometheus server container arguments
  ##
  extraArgs: {}

  ## Additional InitContainers to initialize the pod
  ##
  extraInitContainers: []

  ## Additional Prometheus server Volume mounts
  ##
  extraVolumeMounts: []

  ## Additional Prometheus server Volumes
  ##
  extraVolumes: []

  ## Additional Prometheus server hostPath mounts
  ##
  extraHostPathMounts: []
    # - name: certs-dir
    #   mountPath: /etc/kubernetes/certs
    #   subPath: ""
    #   hostPath: /etc/kubernetes/certs
    #   readOnly: true

  extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /prometheus
    #   subPath: ""
    #   configMap: certs-configmap
    #   readOnly: true

  ## Additional Prometheus server Secret mounts
  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
  extraSecretMounts: []
    # - name: secret-files
    #   mountPath: /etc/secrets
    #   subPath: ""
    #   secretName: prom-secret-files
    #   readOnly: true

  ## ConfigMap override where fullname is .Release.Name-.Values.server.configMapOverrideName
  ## Defining configMapOverrideName will cause templates/server-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configMapOverrideName: ""

  ingress:
    ## If true, Prometheus server Ingress will be created
    ##
    enabled: false

    ## Prometheus server Ingress annotations
    ##
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## Prometheus server Ingress additional labels
    ##
    extraLabels: {}

    ## Prometheus server Ingress hostnames with optional path
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - prometheus.domain.com
    #   - domain.com/prometheus

    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /*
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation

    ## Prometheus server Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-server-tls
    #     hosts:
    #       - prometheus.domain.com

  ## Server Deployment Strategy type
  # strategy:
  #   type: Recreate

  ## hostAliases allows adding entries to /etc/hosts inside the containers
  hostAliases: []
  #   - ip: "127.0.0.1"
  #     hostnames:
  #       - "example.com"

  ## Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for Prometheus server pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Pod affinity
  ##
  affinity: {}

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  ##
  podDisruptionBudget:
    enabled: false
    maxUnavailable: 1

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  persistentVolume:
    ## If true, Prometheus server will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Prometheus server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Prometheus server data Persistent Volume annotations
    ##
    annotations: {}

    ## Prometheus server data Persistent Volume existing claim name
    ## Requires server.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## Prometheus server data Persistent Volume mount root path
    ##
    mountPath: /data

    ## Prometheus server data Persistent Volume size
    ##
    size: 30Gi

    ## Prometheus server data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## Prometheus server data Persistent Volume Binding Mode
    ## If defined, volumeBindingMode: <volumeBindingMode>
    ## If undefined (the default) or set to null, no volumeBindingMode spec is
    ##   set, choosing the default mode.
    ##
    # volumeBindingMode: ""

    ## Subdirectory of Prometheus server data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  emptyDir:
    ## Prometheus server emptyDir volume size limit
    ##
    sizeLimit: ""

  ## Annotations to be added to Prometheus server pods
  ##
  podAnnotations: {}
    # iam.amazonaws.com/role: prometheus

  ## Labels to be added to Prometheus server pods
  ##
  podLabels: {}

  ## Prometheus AlertManager configuration
  ##
  alertmanagers:
        - scheme: http
          path_prefix: "{{ observability_subpathAlertmanager }}"
          static_configs:
          - targets:
              - "prometheus-alertmanager.{{ observability_namespace }}"

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)
  ##
  replicaCount: 1

  ## Annotations to be added to deployment
  ##
  deploymentAnnotations: {}

  statefulSet:
    ## If true, use a statefulset instead of a deployment for pod management.
    ## This allows to scale replicas to more than 1 pod
    ##
    enabled: true

    annotations: {}
    labels: {}
    podManagementPolicy: OrderedReady

    ## Alertmanager headless service to use for the statefulset
    ##
    headless:
      annotations: {}
      labels: {}
      servicePort: 80
      ## Enable gRPC port on service to allow auto discovery with thanos-querier
      gRPC:
        enabled: false
        servicePort: 10901
        # nodePort: 10901

  ## Prometheus server readiness and liveness probe initial delay and timeout
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  ##
  readinessProbeInitialDelay: 30
  readinessProbePeriodSeconds: 5
  readinessProbeTimeout: 30
  readinessProbeFailureThreshold: 3
  readinessProbeSuccessThreshold: 1
  livenessProbeInitialDelay: 30
  livenessProbePeriodSeconds: 15
  livenessProbeTimeout: 30
  livenessProbeFailureThreshold: 3
  livenessProbeSuccessThreshold: 1

  ## Prometheus server resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    limits:
      cpu: 2000m
      memory: 2048Mi
    requests:
      cpu: 1000m
      memory: 600Mi

  ## Vertical Pod Autoscaler config
  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
  verticalAutoscaler:
    ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)
    enabled: false
    # updateMode: "Auto"
    # containerPolicies:
    # - containerName: 'prometheus-server'

  ## Security context to be added to server pods
  ##
  securityContext:
    runAsUser: 65534
    runAsNonRoot: true
    runAsGroup: 65534
    fsGroup: 65534

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9090
    sessionAffinity: None
    type: ClusterIP

    ## Enable gRPC port on service to allow auto discovery with thanos-querier
    gRPC:
      enabled: false
      servicePort: 10901
      # nodePort: 10901

    ## If using a statefulSet (statefulSet.enabled=true), configure the
    ## service to connect to a specific replica to have a consistent view
    ## of the data.
    statefulsetReplica:
      enabled: true
      replica: 0

  ## Prometheus server pod termination grace period
  ##
  terminationGracePeriodSeconds: 300

  ## Prometheus data retention period (default if not specified is 15 days)
  ##
  retention: "15d"

pushgateway:
  ## If false, pushgateway will not be installed
  ##
  enabled: false

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  ## pushgateway container name
  ##
  name: pushgateway

  ## pushgateway container image
  ##
  image:
    repository: prom/pushgateway
    tag: v1.2.0
    pullPolicy: IfNotPresent

  ## pushgateway priorityClassName
  ##
  priorityClassName: ""

  ## Additional pushgateway container arguments
  ##
  ## for example: persistence.file: /data/pushgateway.data
  extraArgs: {}

  ## Additional InitContainers to initialize the pod
  ##
  extraInitContainers: []

  ingress:
    ## If true, pushgateway Ingress will be created
    ##
    enabled: false

    ## pushgateway Ingress annotations
    ##
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## pushgateway Ingress hostnames with optional path
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - pushgateway.domain.com
    #   - domain.com/pushgateway

    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /*
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation

    ## pushgateway Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-alerts-tls
    #     hosts:
    #       - pushgateway.domain.com

  ## Node tolerations for pushgateway scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for pushgateway pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to pushgateway pods
  ##
  podAnnotations: {}

  ## Labels to be added to pushgateway pods
  ##
  podLabels: {}

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  replicaCount: 1

  ## Annotations to be added to deployment
  ##
  deploymentAnnotations: {}

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  ##
  podDisruptionBudget:
    enabled: false
    maxUnavailable: 1

  ## pushgateway resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 32Mi
    # requests:
    #   cpu: 10m
    #   memory: 32Mi

  ## Security context to be added to push-gateway pods
  ##
  securityContext:
    runAsUser: 65534
    runAsNonRoot: true

  service:
    annotations:
      prometheus.io/probe: pushgateway
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the pushgateway service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9091
    type: ClusterIP

  ## pushgateway Deployment Strategy type
  # strategy:
  #   type: Recreate

  persistentVolume:
    ## If true, pushgateway will create/use a Persistent Volume Claim
    ##
    enabled: false

    ## pushgateway data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## pushgateway data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## pushgateway data Persistent Volume existing claim name
    ## Requires pushgateway.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## pushgateway data Persistent Volume mount root path
    ##
    mountPath: /data

    ## pushgateway data Persistent Volume size
    ##
    size: 2Gi

    ## pushgateway data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## pushgateway data Persistent Volume Binding Mode
    ## If defined, volumeBindingMode: <volumeBindingMode>
    ## If undefined (the default) or set to null, no volumeBindingMode spec is
    ##   set, choosing the default mode.
    ##
    # volumeBindingMode: ""

    ## Subdirectory of pushgateway data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""


## alertmanager ConfigMap entries
##
alertmanagerFiles:
  alertmanager.yml:
    global: {}
      # slack_api_url: ''

    # receivers:
    # - name: email
    #   email_configs:
    #   - to: "{{ observability_smtp_to }}"
    #     #html: "{{ '{{ template /alertmanager/email.html . }}' }}"
    #     from: "{{ observability_smtp_from }}"
    #     smarthost: "{{ observability_smtp_host }}"
    #     auth_username: "{{ observability_auth_username }}"
    #     auth_identity: "{{ observability_auth_username }}"
    #     auth_password: "{{ observability_auth_password }}"

    # route:
    #   group_wait: 10s
    #   group_interval: 5m
    #   receiver: email
    #   repeat_interval: 3h
## Prometheus server ConfigMap entries
##
serverFiles:

  ## Alerts configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
  alerting_rules.yml:
    groups:
      - name: kubernetes-apps
        rules:
          - alert: KubePodCrashLooping
            annotations:
              message: >-
                Pod {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.pod }}' }} ({{ '{{ $labels.container
                }}' }}) is restarting {{ '{{ printf "%.2f" $value }}' }} times / 5 minutes.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
            expr: >
              rate(kube_pod_container_status_restarts_total{kubernetes_name="prometheus-kube-state-metrics"}[15m])
              * 60 * 5 > 0
            for: 15m
            labels:
              severity: critical
          - alert: KubePodNotReady
            annotations:
              message: >-
                Pod {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.pod }}' }} has been in a
                non-ready state for longer than 15 minutes.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
            expr: |
              sum by (namespace, pod) (
                max by(namespace, pod) (
                  kube_pod_status_phase{kubernetes_name="prometheus-kube-state-metrics", phase=~"Pending|Unknown"}
                ) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (
                  1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})
                )
              ) > 0
            for: 15m
            labels:
              severity: critical
          - alert: KubeDeploymentGenerationMismatch
            annotations:
              message: >-
                Deployment generation for {{ '{{ $labels.namespace }}' }}/{{ '{{
                $labels.deployment }}' }} does not match, this indicates that the
                Deployment has failed but has not been rolled back.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
            expr: |
              kube_deployment_status_observed_generation{kubernetes_name="prometheus-kube-state-metrics"}
                !=
              kube_deployment_metadata_generation{kubernetes_name="prometheus-kube-state-metrics"}
            for: 15m
            labels:
              severity: critical
          - alert: KubeDeploymentReplicasMismatch
            annotations:
              message: >-
                Deployment {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.deployment }}' }} has not
                matched the expected number of replicas for longer than 15 minutes.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
            expr: |
              kube_deployment_spec_replicas{kubernetes_name="prometheus-kube-state-metrics"}
                !=
              kube_deployment_status_replicas_available{kubernetes_name="prometheus-kube-state-metrics"}
            for: 15m
            labels:
              severity: critical
          - alert: KubeStatefulSetReplicasMismatch
            annotations:
              message: >-
                StatefulSet {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.statefulset }}' }} has
                not matched the expected number of replicas for longer than 15
                minutes.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
            expr: |
              kube_statefulset_status_replicas_ready{kubernetes_name="prometheus-kube-state-metrics"}
                !=
              kube_statefulset_status_replicas{kubernetes_name="prometheus-kube-state-metrics"}
            for: 15m
            labels:
              severity: critical
          - alert: KubeStatefulSetGenerationMismatch
            annotations:
              message: >-
                StatefulSet generation for {{ '{{ $labels.namespace }}' }}/{{ '{{
                $labels.statefulset }}' }} does not match, this indicates that the
                StatefulSet has failed but has not been rolled back.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
            expr: |
              kube_statefulset_status_observed_generation{kubernetes_name="prometheus-kube-state-metrics"}
                !=
              kube_statefulset_metadata_generation{kubernetes_name="prometheus-kube-state-metrics"}
            for: 15m
            labels:
              severity: critical
          - alert: KubeStatefulSetUpdateNotRolledOut
            annotations:
              message: >-
                StatefulSet {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.statefulset }}' }} update
                has not been rolled out.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
            expr: |
              max without (revision) (
                kube_statefulset_status_current_revision{kubernetes_name="prometheus-kube-state-metrics"}
                  unless
                kube_statefulset_status_update_revision{kubernetes_name="prometheus-kube-state-metrics"}
              )
                *
              (
                kube_statefulset_replicas{kubernetes_name="prometheus-kube-state-metrics"}
                  !=
                kube_statefulset_status_replicas_updated{kubernetes_name="prometheus-kube-state-metrics"}
              )
            for: 15m
            labels:
              severity: critical
          - alert: KubeDaemonSetRolloutStuck
            annotations:
              message: >-
                Only {{ '{{ $value | humanizePercentage }}' }} of the desired Pods of
                DaemonSet {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.daemonset }}' }} are
                scheduled and ready.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
            expr: >
              kube_daemonset_status_number_ready{kubernetes_name="prometheus-kube-state-metrics"}
                /
              kube_daemonset_status_desired_number_scheduled{kubernetes_name="prometheus-kube-state-metrics"}
              < 1.00
            for: 15m
            labels:
              severity: critical
          - alert: KubeContainerWaiting
            annotations:
              message: >-
                Pod {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.pod }}' }} container {{ '{{
                $labels.container}}' }} has been in waiting state for longer than 1
                hour.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontainerwaiting
            expr: >
              sum by (namespace, pod, container)
              (kube_pod_container_status_waiting_reason{kubernetes_name="prometheus-kube-state-metrics"}) >
              0
            for: 1h
            labels:
              severity: warning
          - alert: KubeDaemonSetNotScheduled
            annotations:
              message: >-
                {{ '{{ $value }}' }} Pods of DaemonSet {{ '{{ $labels.namespace }}' }}/{{ '{{
                $labels.daemonset }}' }} are not scheduled.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
            expr: >
              kube_daemonset_status_desired_number_scheduled{kubernetes_name="prometheus-kube-state-metrics"}
                -
              kube_daemonset_status_current_number_scheduled{kubernetes_name="prometheus-kube-state-metrics"}
              > 0
            for: 10m
            labels:
              severity: warning
          - alert: KubeDaemonSetMisScheduled
            annotations:
              message: >-
                {{ '{{ $value }}' }} Pods of DaemonSet {{ '{{ $labels.namespace }}' }}/{{ '{{
                $labels.daemonset }}' }} are running where they are not supposed to run.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
            expr: >
              kube_daemonset_status_number_misscheduled{kubernetes_name="prometheus-kube-state-metrics"} >
              0
            for: 10m
            labels:
              severity: warning
          - alert: KubeCronJobRunning
            annotations:
              message: >-
                CronJob {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.cronjob }}' }} is taking more
                than 1h to complete.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning
            expr: >
              time() - kube_cronjob_next_schedule_time{kubernetes_name="prometheus-kube-state-metrics"} >
              3600
            for: 1h
            labels:
              severity: warning
          - alert: KubeJobCompletion
            annotations:
              message: >-
                Job {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.job_name }}' }} is taking more
                than one hour to complete.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
            expr: >
              kube_job_spec_completions{kubernetes_name="prometheus-kube-state-metrics"} -
              kube_job_status_succeeded{kubernetes_name="prometheus-kube-state-metrics"}  > 0
            for: 1h
            labels:
              severity: warning
          - alert: KubeJobFailed
            annotations:
              message: >-
                Job {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.job_name }}' }} failed to
                complete.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
            expr: |
              kube_job_failed{kubernetes_name="prometheus-kube-state-metrics"}  > 0
            for: 15m
            labels:
              severity: warning
          - alert: KubeHpaReplicasMismatch
            annotations:
              message: >-
                HPA {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.hpa }}' }} has not matched the
                desired number of replicas for longer than 15 minutes.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpareplicasmismatch
            expr: |
              (kube_hpa_status_desired_replicas{kubernetes_name="prometheus-kube-state-metrics"}
                !=
              kube_hpa_status_current_replicas{kubernetes_name="prometheus-kube-state-metrics"})
                and
              changes(kube_hpa_status_current_replicas[15m]) == 0
            for: 15m
            labels:
              severity: warning
          - alert: KubeHpaMaxedOut
            annotations:
              message: >-
                HPA {{ '{{ $labels.namespace }}' }}/{{ '{{ $labels.hpa }}' }} has been running at
                max replicas for longer than 15 minutes.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpamaxedout
            expr: |
              kube_hpa_status_current_replicas{kubernetes_name="prometheus-kube-state-metrics"}
                ==
              kube_hpa_spec_max_replicas{kubernetes_name="prometheus-kube-state-metrics"}
            for: 15m
            labels:
              severity: warning
      - name: kubernetes-resources
        rules:
          - alert: KubeCPUOvercommit
            annotations:
              message: >-
                Cluster has overcommitted CPU resource requests for Pods and cannot
                tolerate node failure.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
            expr: >
              sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum)
                /
              sum(kube_node_status_allocatable_cpu_cores)
                >
              (count(kube_node_status_allocatable_cpu_cores)-1) /
              count(kube_node_status_allocatable_cpu_cores)
            for: 5m
            labels:
              severity: warning
          - alert: KubeMemOvercommit
            annotations:
              message: >-
                Cluster has overcommitted memory resource requests for Pods and
                cannot tolerate node failure.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
            expr: |
              sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
                /
              sum(kube_node_status_allocatable_memory_bytes)
                >
              (count(kube_node_status_allocatable_memory_bytes)-1)
                /
              count(kube_node_status_allocatable_memory_bytes)
            for: 5m
            labels:
              severity: warning
          - alert: KubeCPUOvercommit
            annotations:
              message: Cluster has overcommitted CPU resource requests for Namespaces.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
            expr: >
              sum(kube_resourcequota{kubernetes_name="prometheus-kube-state-metrics", type="hard",
              resource="cpu"})
                /
              sum(kube_node_status_allocatable_cpu_cores)
                > 1.5
            for: 5m
            labels:
              severity: warning
          - alert: KubeMemOvercommit
            annotations:
              message: Cluster has overcommitted memory resource requests for Namespaces.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
            expr: >
              sum(kube_resourcequota{kubernetes_name="prometheus-kube-state-metrics", type="hard",
              resource="memory"})
                /
              sum(kube_node_status_allocatable_memory_bytes{component="node-exporter"})
                > 1.5
            for: 5m
            labels:
              severity: warning
          - alert: KubeQuotaFullyUsed
            annotations:
              message: >-
                Namespace {{ '{{ $labels.namespace }}' }} is using {{ '{{ $value |
                humanizePercentage }}' }} of its {{ '{{ $labels.resource }}' }} quota.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotafullyused
            expr: |
              kube_resourcequota{kubernetes_name="prometheus-kube-state-metrics", type="used"}
                / ignoring(instance, job, type)
              (kube_resourcequota{kubernetes_name="prometheus-kube-state-metrics", type="hard"} > 0)
                >= 1
            for: 15m
            labels:
              severity: info
          - alert: CPUThrottlingHigh
            annotations:
              message: >-
                {{ '{{ $value | humanizePercentage }}' }} throttling of CPU in namespace {{ '{{
                $labels.namespace }}' }} for container {{ '{{ $labels.container }}' }} in pod {{ '{{
                $labels.pod }}' }}.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh
            expr: >
              sum(increase(container_cpu_cfs_throttled_periods_total{container!="",
              }[5m])) by (container, pod, namespace)
                /
              sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container,
              pod, namespace)
                > ( 25 / 100 )
            for: 15m
            labels:
              severity: warning
      - name: kubernetes-storage
        rules:
          - alert: KubePersistentVolumeUsageCritical
            annotations:
              message: >-
                The PersistentVolume claimed by {{ '{{ $labels.persistentvolumeclaim }}' }}
                in Namespace {{ '{{ $labels.namespace }}' }} is only {{ '{{ $value |
                humanizePercentage }}' }} free.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
            expr: |
              kubelet_volume_stats_available_bytes{job="kubernetes-nodes"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubernetes-nodes"}
                < 0.03
            for: 1m
            labels:
              severity: critical
          - alert: KubePersistentVolumeFullInFourDays
            annotations:
              message: >-
                Based on recent sampling, the PersistentVolume claimed by {{ '{{
                $labels.persistentvolumeclaim }}' }} in Namespace {{ '{{ $labels.namespace
                }}' }} is expected to fill up within four days. Currently {{ '{{ $value |
                humanizePercentage }}' }} is available.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays
            expr: >
              (
                kubelet_volume_stats_available_bytes{job="kubernetes-nodes"}
                  /
                kubelet_volume_stats_capacity_bytes{job="kubernetes-nodes"}
              ) < 0.15

              and

              predict_linear(kubelet_volume_stats_available_bytes{job="kubernetes-nodes"}[6h],
              4 * 24 * 3600) < 0
            for: 1h
            labels:
              severity: critical
          - alert: KubePersistentVolumeErrors
            annotations:
              message: >-
                The persistent volume {{ '{{ $labels.persistentvolume }}' }} has status {{ '{{
                $labels.phase }}' }}.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
            expr: >
              kube_persistentvolume_status_phase{phase=~"Failed|Pending",kubernetes_name="prometheus-kube-state-metrics"}
              > 0
            for: 5m
            labels:
              severity: critical
      - name: kubernetes-system
        rules:
          - alert: KubeVersionMismatch
            annotations:
              message: >-
                There are {{ '{{ $value }}' }} different semantic versions of Kubernetes
                components running.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
            expr: >
              count(count by (gitVersion)
              (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*")))
              > 1
            for: 15m
            labels:
              severity: warning
          - alert: KubeClientErrors
            annotations:
              message: >-
                Kubernetes API server client '{{ '{{ $labels.job }}' }}/{{ '{{ $labels.instance
                }}' }}' is experiencing {{ '{{ $value | humanizePercentage }}' }} errors.'
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
            expr: >
              (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance,
              job)
                /
              sum(rate(rest_client_requests_total[5m])) by (instance, job))

              > 0.01
            for: 15m
            labels:
              severity: warning
      - name: kube-apiserver-error
        rules:
          - alert: ErrorBudgetBurn
            annotations:
              message: >-
                High error budget burn for job=kubernetes-apiservers (current value: {{ '{{
                $value }}' }})
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn
            expr: >
              sum(apiserver_request_total:burnrate5m{job="kubernetes-apiservers"}) > (14.40
              * (1-0.99000))

              and

              sum(apiserver_request_total:burnrate1h{job="kubernetes-apiservers"}) > (14.40
              * (1-0.99000))
            for: 2m
            labels:
              job: kube-apiserver
              severity: critical
          - alert: ErrorBudgetBurn
            annotations:
              message: >-
                High error budget burn for job=kubernetes-apiservers (current value: {{ '{{
                $value }}' }})
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn
            expr: >
              sum(apiserver_request_total:burnrate30m{job="kubernetes-apiservers"}) > (6.00
              * (1-0.99000))

              and

              sum(apiserver_request_total:burnrate6h{job="kubernetes-apiservers"}) > (6.00
              * (1-0.99000))
            for: 15m
            labels:
              job: kube-apiserver
              severity: critical
          - alert: ErrorBudgetBurn
            annotations:
              message: >-
                High error budget burn for job=kubernetes-apiservers (current value: {{ '{{
                $value }}' }})
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn
            expr: >
              sum(apiserver_request_total:burnrate2h{job="kubernetes-apiservers"}) > (3.00
              * (1-0.99000))

              and

              sum(apiserver_request_total:burnrate1d{job="kubernetes-apiservers"}) > (3.00
              * (1-0.99000))
            for: 1h
            labels:
              job: kube-apiserver
              severity: warning
          - alert: ErrorBudgetBurn
            annotations:
              message: >-
                High error budget burn for job=kubernetes-apiservers (current value: {{ '{{
                $value }}' }})
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn
            expr: >
              sum(apiserver_request_total:burnrate6h{job="kubernetes-apiservers"}) > (1.00
              * (1-0.99000))

              and

              sum(apiserver_request_total:burnrate3d{job="kubernetes-apiservers"}) > (1.00
              * (1-0.99000))
            for: 3h
            labels:
              job: kube-apiserver
              severity: warning
          - expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[1d]))

              /

              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[1d]))
            labels:
              job: kube-apiserver
            record: 'apiserver_request_total:burnrate1d'
          - expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[1h]))

              /

              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[1h]))
            labels:
              job: kube-apiserver
            record: 'apiserver_request_total:burnrate1h'
          - expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[2h]))

              /

              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[2h]))
            labels:
              job: kube-apiserver
            record: 'apiserver_request_total:burnrate2h'
          - expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[30m]))

              /

              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[30m]))
            labels:
              job: kube-apiserver
            record: 'apiserver_request_total:burnrate30m'
          - expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[3d]))

              /

              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[3d]))
            labels:
              job: kube-apiserver
            record: 'apiserver_request_total:burnrate3d'
          - expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[5m]))

              /

              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[5m]))
            labels:
              job: kube-apiserver
            record: 'apiserver_request_total:burnrate5m'
          - expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[6h]))

              /

              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[6h]))
            labels:
              job: kube-apiserver
            record: 'apiserver_request_total:burnrate6h'
      - name: kubernetes-system-apiserver
        rules:
          - alert: KubeAPILatencyHigh
            annotations:
              message: >-
                The API server has an abnormal latency of {{ '{{ $value }}' }} seconds for
                {{ '{{ $labels.verb }}' }} {{ '{{ $labels.resource }}' }}.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
            expr: >
              cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="kubernetes-apiservers",quantile="0.99"}

              >

              1

              and on (verb,resource)

              (
                cluster:apiserver_request_duration_seconds:mean5m{job="kubernetes-apiservers"}
                >
                on (verb) group_left()
                (
                  avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kubernetes-apiservers"} >= 0)
                  +
                  2*stddev by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kubernetes-apiservers"} >= 0)
                )
              ) > on (verb) group_left()

              1.2 * avg by (verb)
              (cluster:apiserver_request_duration_seconds:mean5m{job="kubernetes-apiservers"}
              >= 0)
            for: 5m
            labels:
              severity: warning
          - alert: KubeAPILatencyHigh
            annotations:
              message: >-
                The API server has a 99th percentile latency of {{ '{{ $value }}' }} seconds
                for {{ '{{ $labels.verb }}' }} {{ '{{ $labels.resource }}' }}.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
            expr: >
              cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="kubernetes-apiservers",quantile="0.99"}
              > 4
            for: 10m
            labels:
              severity: critical
          - alert: KubeAPIErrorsHigh
            annotations:
              message: >-
                API server is returning errors for {{ '{{ $value | humanizePercentage }}' }}
                of requests.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
            expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[5m]))
                /
              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[5m])) > 0.03
            for: 10m
            labels:
              severity: critical
          - alert: KubeAPIErrorsHigh
            annotations:
              message: >-
                API server is returning errors for {{ '{{ $value | humanizePercentage }}' }}
                of requests.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
            expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[5m]))
                /
              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[5m])) > 0.01
            for: 10m
            labels:
              severity: warning
          - alert: KubeAPIErrorsHigh
            annotations:
              message: >-
                API server is returning errors for {{ '{{ $value | humanizePercentage }}' }}
                of requests for {{ '{{ $labels.verb }}' }} {{ '{{ $labels.resource }}' }} {{ '{{
                $labels.subresource }}' }}.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
            expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[5m]))
              by (resource,subresource,verb)
                /
              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[5m])) by
              (resource,subresource,verb) > 0.10
            for: 10m
            labels:
              severity: critical
          - alert: KubeAPIErrorsHigh
            annotations:
              message: >-
                API server is returning errors for {{ '{{ $value | humanizePercentage }}' }}
                of requests for {{ '{{ $labels.verb }}' }} {{ '{{ $labels.resource }}' }} {{ '{{
                $labels.subresource }}' }}.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
            expr: >
              sum(rate(apiserver_request_total{job="kubernetes-apiservers",code=~"5.."}[5m]))
              by (resource,subresource,verb)
                /
              sum(rate(apiserver_request_total{job="kubernetes-apiservers"}[5m])) by
              (resource,subresource,verb) > 0.05
            for: 10m
            labels:
              severity: warning
          - alert: KubeClientCertificateExpiration
            annotations:
              message: >-
                A client certificate used to authenticate to the apiserver is
                expiring in less than 7.0 days.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
            expr: >
              apiserver_client_certificate_expiration_seconds_count{job="kubernetes-apiservers"}
              > 0 and histogram_quantile(0.01, sum by (job, le)
              (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubernetes-apiservers"}[5m])))
              < 604800
            labels:
              severity: warning
          - alert: KubeClientCertificateExpiration
            annotations:
              message: >-
                A client certificate used to authenticate to the apiserver is
                expiring in less than 24.0 hours.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
            expr: >
              apiserver_client_certificate_expiration_seconds_count{job="kubernetes-apiservers"}
              > 0 and histogram_quantile(0.01, sum by (job, le)
              (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubernetes-apiservers"}[5m])))
              < 86400
            labels:
              severity: critical
          - alert: AggregatedAPIErrors
            annotations:
              message: >-
                An aggregated API {{ '{{ $labels.name }}' }}/{{ '{{ $labels.namespace }}' }} has
                reported errors. The number of errors have increased for it in the
                past five minutes. High values indicate that the availability of the
                service changes too often.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapierrors
            expr: >
              sum by(name,
              namespace)(increase(aggregator_unavailable_apiservice_count[5m])) > 2
            labels:
              severity: warning
          - alert: KubeAPIDown
            annotations:
              message: KubeAPI has disappeared from Prometheus target discovery.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown
            expr: |
              absent(up{job="kubernetes-apiservers"} == 1)
            for: 15m
            labels:
              severity: critical
      - name: kubernetes-system-kubelet
        rules:
          - alert: KubeNodeNotReady
            annotations:
              message: '{{ '{{ $labels.node }}' }} has been unready for more than 15 minutes.'
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
            expr: >
              kube_node_status_condition{kubernetes_name="prometheus-kube-state-metrics",condition="Ready",status="true"}
              == 0
            for: 15m
            labels:
              severity: warning
          - alert: KubeNodeUnreachable
            annotations:
              message: >-
                {{ '{{ $labels.node }}' }} is unreachable and some workloads may be
                rescheduled.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable
            expr: >
              kube_node_spec_taint{kubernetes_name="prometheus-kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"}
              == 1
            labels:
              severity: warning
          - alert: KubeletTooManyPods
            annotations:
              message: >-
                Kubelet '{{ '{{ $labels.node }}' }}' is running at {{ '{{ $value |
                humanizePercentage }}' }} of its Pod capacity.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
            expr: >
              max(max(kubelet_running_pod_count{job="kubernetes-nodes"}) by(instance) *
              on(instance) group_left(node) kubelet_node_name{job="kubernetes-nodes"})
              by(node) /
              max(kube_node_status_capacity_pods{kubernetes_name="prometheus-kube-state-metrics"}) by(node)
              > 0.95
            for: 15m
            labels:
              severity: warning
          - alert: KubeletDown
            annotations:
              message: Kubelet has disappeared from Prometheus target discovery.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown
            expr: |
              absent(up{job="kubernetes-nodes"} == 1)
            for: 15m
            labels:
              severity: critical
      - name: kubernetes-system-controller-manager
        rules:
          - alert: KubeControllerManagerDown
            annotations:
              message: >-
                KubeControllerManager has disappeared from Prometheus target
                discovery.
              runbook_url: >-
                https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown
            expr: |
              absent(kube_pod_container_status_ready{container="openstack-cloud-controller-manager"} == 1)
            for: 15m
            labels:
              severity: critical


  # groups:
  #   - name: Instances
  #     rules:
  #       - alert: InstanceDown
  #         expr: up == 0
  #         for: 5m
  #         labels:
  #           severity: page
  #         annotations:
  #           description: ' $labels.instance  of job  $labels.job  has been down for more than 5 minutes.'
  #           summary: 'Instance  $labels.instance  down'
  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml
  alerts: {}

  ## Records configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/
  recording_rules.yml:
      groups:
        - name: kube-apiserver.rules
          rules:
            - expr: >
                sum(rate(apiserver_request_duration_seconds_sum{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m]))
                without(instance, pod)

                /

                sum(rate(apiserver_request_duration_seconds_count{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m]))
                without(instance, pod)
              record: 'cluster:apiserver_request_duration_seconds:mean5m'
            - expr: >
                histogram_quantile(0.99,
                sum(rate(apiserver_request_duration_seconds_bucket{job="kubernetes-apiservers",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.99'
              record: 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile'
            - expr: >
                histogram_quantile(0.9,
                sum(rate(apiserver_request_duration_seconds_bucket{job="kubernetes-apiservers",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.9'
              record: 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile'
            - expr: >
                histogram_quantile(0.5,
                sum(rate(apiserver_request_duration_seconds_bucket{job="kubernetes-apiservers",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.5'
              record: 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile'
        - name: k8s.rules
          rules:
            - expr: >
                sum(rate(container_cpu_usage_seconds_total{ job="kubernetes-nodes-cadvisor", image!="",
                container!="POD"}[5m])) by (namespace)
              record: 'namespace:container_cpu_usage_seconds_total:sum_rate'
            - expr: >
                sum by (cluster, namespace, pod, container) (
                  rate(container_cpu_usage_seconds_total{ job="kubernetes-nodes-cadvisor", image!="", container!="POD"}[5m])
                ) * on (cluster, namespace, pod) group_left(node) topk by (cluster,
                namespace, pod) (
                  1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: >-
                node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
            - expr: |
                container_memory_working_set_bytes{ job="kubernetes-nodes-cadvisor", image!=""}
                * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
                  max by(namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: 'node_namespace_pod_container:container_memory_working_set_bytes'
            - expr: |
                container_memory_rss{ job="kubernetes-nodes-cadvisor", image!=""}
                * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
                  max by(namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: 'node_namespace_pod_container:container_memory_rss'
            - expr: |
                container_memory_cache{ job="kubernetes-nodes-cadvisor", image!=""}
                * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
                  max by(namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: 'node_namespace_pod_container:container_memory_cache'
            - expr: |
                container_memory_swap{ job="kubernetes-nodes-cadvisor", image!=""}
                * on (namespace, pod) group_left(node) topk by(namespace, pod) (1,
                  max by(namespace, pod, node) (kube_pod_info{node!=""})
                )
              record: 'node_namespace_pod_container:container_memory_swap'
            - expr: >
                sum(container_memory_usage_bytes{ job="kubernetes-nodes-cadvisor", image!="",
                container!="POD"}) by (namespace)
              record: 'namespace:container_memory_usage_bytes:sum'
            - expr: |
                sum by (namespace) (
                    sum by (namespace, pod) (
                        max by (namespace, pod, container) (
                            kube_pod_container_resource_requests_memory_bytes{kubernetes_name="prometheus-kube-state-metrics"}
                        ) * on(namespace, pod) group_left() max by (namespace, pod) (
                            kube_pod_status_phase{phase=~"Pending|Running"} == 1
                        )
                    )
                )
              record: 'namespace:kube_pod_container_resource_requests_memory_bytes:sum'
            - expr: |
                sum by (namespace) (
                    sum by (namespace, pod) (
                        max by (namespace, pod, container) (
                            kube_pod_container_resource_requests_cpu_cores{kubernetes_name="prometheus-kube-state-metrics"}
                        ) * on(namespace, pod) group_left() max by (namespace, pod) (
                          kube_pod_status_phase{phase=~"Pending|Running"} == 1
                        )
                    )
                )
              record: 'namespace:kube_pod_container_resource_requests_cpu_cores:sum'
            - expr: |
                max by (cluster, namespace, workload, pod) (
                  label_replace(
                    label_replace(
                      kube_pod_owner{kubernetes_name="prometheus-kube-state-metrics", owner_kind="ReplicaSet"},
                      "replicaset", "$1", "owner_name", "(.*)"
                    ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
                      1, max by (replicaset, namespace, owner_name) (
                        kube_replicaset_owner{kubernetes_name="prometheus-kube-state-metrics"}
                      )
                    ),
                    "workload", "$1", "owner_name", "(.*)"
                  )
                )
              labels:
                workload_type: deployment
              record: mixin_pod_workload
            - expr: |
                max by (cluster, namespace, workload, pod) (
                  label_replace(
                    kube_pod_owner{kubernetes_name="prometheus-kube-state-metrics", owner_kind="DaemonSet"},
                    "workload", "$1", "owner_name", "(.*)"
                  )
                )
              labels:
                workload_type: daemonset
              record: mixin_pod_workload
            - expr: |
                max by (cluster, namespace, workload, pod) (
                  label_replace(
                    kube_pod_owner{kubernetes_name="prometheus-kube-state-metrics", owner_kind="StatefulSet"},
                    "workload", "$1", "owner_name", "(.*)"
                  )
                )
              labels:
                workload_type: statefulset
              record: mixin_pod_workload
        - name: kube-scheduler.rules
          rules:
            - expr: >
                histogram_quantile(0.99,
                sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.99'
              record: >-
                cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.99,
                sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.99'
              record: >-
                cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.99,
                sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.99'
              record: 'cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile'
            - expr: >
                histogram_quantile(0.9,
                sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.9'
              record: >-
                cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.9,
                sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.9'
              record: >-
                cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.9,
                sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.9'
              record: 'cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile'
            - expr: >
                histogram_quantile(0.5,
                sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.5'
              record: >-
                cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.5,
                sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.5'
              record: >-
                cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.5,
                sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.5'
              record: 'cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile'
        - name: node.rules
          rules:
            - expr: |
                sum(min(kube_pod_info{node!=""}) by (cluster, node))
              record: ':kube_pod_info_node_count:'
            - expr: |
                topk by(namespace, pod) (1,
                  max by (node, namespace, pod) (
                    label_replace(kube_pod_info{kubernetes_name="prometheus-kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
                ))
              record: 'node_namespace_pod:kube_pod_info:'
            - expr: |
                count by (cluster, node) (sum by (node, cpu) (
                  node_cpu_seconds_total{component="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                ))
              record: 'node:node_num_cpu:sum'
            - expr: |
                sum(
                  node_memory_MemAvailable_bytes{component="node-exporter"} or
                  (
                    node_memory_Buffers_bytes{component="node-exporter"} +
                    node_memory_Cached_bytes{component="node-exporter"} +
                    node_memory_MemFree_bytes{component="node-exporter"} +
                    node_memory_Slab_bytes{component="node-exporter"}
                  )
                ) by (cluster)
              record: ':node_memory_MemAvailable_bytes:sum'

  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml
  rules: {}

  prometheus.yml:
    rule_files:
      - /etc/config/recording_rules.yml
      - /etc/config/alerting_rules.yml
    ## Below two files are DEPRECATED will be removed from this default values file
      - /etc/config/rules
      - /etc/config/alerts

    scrape_configs:
      - job_name: prometheus
        static_configs:
          - targets:
            - localhost:9090

      # A scrape configuration for running Prometheus on a Kubernetes cluster.
      # This uses separate scrape configs for cluster components (i.e. API server, node)
      # and services to allow each to use different authentication configs.
      #
      # Kubernetes labels will be added as Prometheus labels on metrics via the
      # `labelmap` relabeling action.

      # Scrape config for API servers.
      #
      # Kubernetes exposes API servers as endpoints to the default/kubernetes
      # service so this uses `endpoints` role and uses relabelling to only keep
      # the endpoints associated with the default/kubernetes service using the
      # default named port `https`. This works for single API server deployments as
      # well as HA API server deployments.
      - job_name: 'kubernetes-apiservers'

        kubernetes_sd_configs:
          - role: endpoints

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        # Keep only the default/kubernetes service endpoints for the https port. This
        # will add targets for each API server which Kubernetes adds an endpoint to
        # the default/kubernetes service.
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      - job_name: 'kubernetes-nodes'

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
          - role: node

        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$1/proxy/metrics


      - job_name: 'kubernetes-nodes-cadvisor'

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
          - role: node

        # This configuration will work only on kubelet 1.7.3+
        # As the scrape endpoints for cAdvisor have changed
        # if you are using older version you need to change the replacement to
        # replacement: /api/v1/nodes/$1:4194/proxy/metrics
        # more info here https://github.com/coreos/prometheus-operator/issues/633
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: 'kubernetes-service-endpoints'

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: kubernetes_node

      # Scrape config for slow service endpoints; same as above, but with a larger
      # timeout and a larger interval
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: 'kubernetes-service-endpoints-slow'

        scrape_interval: 5m
        scrape_timeout: 30s

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: kubernetes_node

      - job_name: 'prometheus-pushgateway'
        honor_labels: true

        kubernetes_sd_configs:
          - role: service

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: pushgateway

      # Example scrape config for probing services via the Blackbox Exporter.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      - job_name: 'kubernetes-services'

        metrics_path: /probe
        params:
          module: [http_2xx]

        kubernetes_sd_configs:
          - role: service

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: true
          - source_labels: [__address__]
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name

      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods'

        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
          - source_labels: [__meta_kubernetes_pod_phase]
            regex: Pending|Succeeded|Failed
            action: drop

      # Example Scrape config for pods which should be scraped slower. An useful example
      # would be stackriver-exporter which querys an API on every scrape of the pod
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods-slow'

        scrape_interval: 5m
        scrape_timeout: 30s

        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
          - source_labels: [__meta_kubernetes_pod_phase]
            regex: Pending|Succeeded|Failed
            action: drop

      ## custom

# adds additional scrape configs to prometheus.yml
# must be a string so you have to add a | after extraScrapeConfigs:
# example adds prometheus-blackbox-exporter scrape config
extraScrapeConfigs:
  # - job_name: 'prometheus-blackbox-exporter'
  #   metrics_path: /probe
  #   params:
  #     module: [http_2xx]
  #   static_configs:
  #     - targets:
  #       - https://example.com
  #   relabel_configs:
  #     - source_labels: [__address__]
  #       target_label: __param_target
  #     - source_labels: [__param_target]
  #       target_label: instance
  #     - target_label: __address__
  #       replacement: prometheus-blackbox-exporter:9115

# Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager
# useful in H/A prometheus with different external labels but the same alerts
alertRelabelConfigs:
  # alert_relabel_configs:
  # - source_labels: [dc]
  #   regex: (.+)\d+
  #   target_label: dc

networkPolicy:
  ## Enable creation of NetworkPolicy resources.
  ##
  enabled: false

# Force namespace of namespaced resources
forceNamespace: null